\section{Aufgabenblatt 10}

\subsection{Assignment 1}
	\subsubsection*{Blocking and non-blocking operations}
		The lecture slides define a blocking operation in the following way: 
		\textit{'Blocking: The return of the operation on a process ensures that the local resources (buffers) can be reused.'}
		That means that a process cannot perform any other operation on the local buffers until the previous, blocking operation completly finished and the buffer is free to be reused. 
		
		In contradiction, \textit{'a non-blocking operation does not guarantee that the local resources can be reused after returning to the process.} That results into a behaviour that the process, that calls an operation cannot be sure if the buffer is reusable again because the operation instantly returns the control flow to the process.
		
		The standard \texttt{MPI\_Send()} and \texttt{MPI\_Recv()} are blocking operations.

		
	\subsubsection*{Collective and point-to-point communication}
		Collective communication means that all processes in a MPI communicator	take part in it.  
		Point-to-point means that the conversation just consists of a dialog between peers.
		The standard \texttt{MPI\_Send()} and \texttt{MPI\_Recv()} use point-to-point communication. 
		The functions \texttt{MPI\_Bcast()}, \texttt{MPI\_Reduce()}, \texttt{MPI\_Gather()}, \texttt{MPI\_Scatter()}, \texttt{MPI\_Allgather()} and \texttt{MPI\_Alltoall()} offer collective communication where a source can reach more than one target and vice versa.
	
\subsection{Assignment 2}
	\subsubsection*{Running the hello world example}
	After running the example from helloworld.c the output looked like this:
\begin{verbatim}
Hello MPI from the server process!
Hello MPI!
 mesg from 1 of 8 on c008
Hello MPI!
 mesg from 2 of 8 on c007
Hello MPI!
 mesg from 3 of 8 on c006
Hello MPI!
 mesg from 4 of 8 on c005
Hello MPI!
 mesg from 5 of 8 on c004
Hello MPI!
 mesg from 6 of 8 on c002
Hello MPI!
 mesg from 7 of 8 on c001
\end{verbatim}
	Here we can see, that first of all the process with rank 0 declares himself as ``server process". It then waits for incoming messages on each of the other ranks by calling a blocking \texttt{MPI\_Recv}. All the other processes having not rank 0 will send its hostname to the server process. The communication is held by point-to-point dialogs from each client process to the server process.
	\subsubsection*{Scaling} 
	The maximum amount of processes we can spawn is determined by the number of available/free nodes in the cluster. After setting the number of processes to $available nodes + 1$ the job was held in the queue and not executed.


	\subsection{Assignment 3}
		The solution of assignment 3 can be seen in the following listing:

		\lstinputlisting[language=C]{./task_10/single_ring_blocking.c}
		The output of this program looks like this:

\begin{verbatim}	
Process 0 sent message to process 1
Process 1 received message from process 0
Process 1 sent message to process 2
---Process sum is 1
Process 2 received message from process 1
Process 2 sent message to process 3
---Process sum is 3
Process 3 received message from process 2
Process 3 sent message to process 4
---Process sum is 6
Process 4 received message from process 3
Process 4 sent message to process 5
---Process sum is 10
Process 5 received message from process 4
Process 5 sent message to process 6
---Process sum is 15
Process 6 received message from process 5
Process 6 sent message to process 7
---Process sum is 21
Process 7 received message from process 6
Process 7 sent message to process 0
---Process sum is 28
Process 0 received message from process 7
---Process sum is 28
\end{verbatim}

	\subsection{Assignment 4}
		
		The solution of assignment 3 can be seen in the following listing:
		\lstinputlisting[language=C]{./task_10/multi_pass_blocking.c}


	\subsection{Assignment 5}
		The deadlock occurs because the two processors try to send and receive at the same time.
		The sending operation operates synchronous,
		so it will wait for the other process to receive the message.
		As this happens on both processors,
		the deadlock occurs.
		A more simple solution than ours would be to use \texttt{MPI\_send},
		which is asynchronous,
		but than we need to assume that the system buffer deals with the problem.
		But if we run out of system buffer,
		the deadlock would still occur.
		
		Therefore we propose the following solution.
		\lstinputlisting[language=C]{./task_10/deadlock.c}

